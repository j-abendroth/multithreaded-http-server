###### John Abendroth
###### CruzID: jabendro
# Load Balancer Design Document

## Introduction
The program `loadbalancer` is designed to balance requests between multiple copies of asgn2's `httpserver`. It connects as a client to the different copies of `httpserver` and bridges data between the clients connected to it and the servers.

## Design
### Healthcheck Probing

The first major part of my loadbalancer is how I handled healthcheck probing the servers. As apart of the spec, the loadbalancer is to probe all the specified servers at either -R requests, or X seconds. For my implementation, X is 2 seconds across the program. Thus, my healthcheck probes the servers every 2 seconds, when there have been -R requests accepted, whichever is first.

My healthcheck design begins with a shared healthcheckObject struct, which holds data fields like the request rate specified, the current optimal server port, the current request counter, and a shared array of serverObjects for each server port specified.

In the main() function, I create a main healthchecking thread that is responsible for running its infinite loop and probing the specified servers as necessary. However, before the main healthcheck thread enters its main `while(1)` loop, it breaks out to a `first_probe` function that is locked by semaphores. This is to ensure that the first healthcheck probe is completed before the loadbalancer calls `accept()` on any requests. The `first_probe` function creates -N number of small worker threads who then go and probe their respective servers, by having one `serverObject` passed into them from the main array of `serverObject`. The `first_probe` function sends a `GET /healthcheck` request to its server, and then uses `select()` to time for a response. If 2 seconds pass with no response, the server is marked as down. It then receives the `healthcheck` response from the server and parses it, also validating the response. If anything is wrong with the response, the server is marked as down and the worker thread returns to the main `healthcheck` thread. Once the `first_probe` function returns, the semaphore is put down and the healthcheck thread enters its main `while(1)` loop. Here it utilizes `pthread_cond_timedwait()` to wait on a condition variable signaled by the `main()` that there have been R requests, or that 2 seconds have past, whichever is first. It then does the same thing as the `first_probe` function, creating -N mini threads to probe their server and update its entry in the array, validating the `healthcheck` response from the server. Finally, it loops through the array of `serverObject` and looks for the current minimum number of entries for servers that are marked as operational, and sets that server port number as a global optimal port number to use until the next `healthcheck`probe is complete. If no valid minimum number of entries is found, the `healthcheck` thread sets the global optimal port number to -1 as a sign that all servers are down, and to return a 500 response code to any client that sends a response until the next `healthcheck` probe.

### Data Bridging

The second major part of the loadbalancer is how it bridges data between its clients and the servers specified. I re-used my main design from asgn2 with a producer-consumer queue setup and worker threads. In `main()`, I setup an array of worker threads with the size of the number of parallel connections specified. Then `main()` enters its `while(1)` loop and just keeps accepting connections, putting the connection file descriptors into a specific worker thread queue and then signaling that specific worker thread on its condition variable. The `Enqueue()` operation is locked to be atomic, so that the worker thread can't access the queue and dequeue something while a connection is being enqueued.

In the worker threads, they sit on an infinite `while(1)` loop, and begin by attempting to take the queue lock and dequeue something. They'll wait on `pthread_cond_wait()` if the queue is empty, otherwise it will go ahead and dequeue a connection. The worker thread then atomically grabs the current shared optimal server port number, and attempts to connect to it with the start code function `client_connect`. If the connection fails, or the optimal server port returned is -1, the worker thread sends a 500 error response to the client and ends the connection. Otherwise, it calls the starter code `bridge_loop()` on the client socket fd and the server connection fd, where `bridge_loop()` continues to to send data between the 2 file descriptor sockets until the return value is 0, at which point there is no more data to send. The worker thread then closes the connection and waits for a new connection on its queue.

### Min Entries Algorithm

The algorithm I used to determine the minimum number of entries between the servers marked operational is as follows:

I first check if the status flag of the server[i] is marked as operational. If it does not equal 1, the loop continues so that that server isn't considered for the optimal server port. Then, I check if the local variable tracking the min entries is set to a dummy number of -1, indicating it hasn't been set yet. If it is equal to -1, then the current entry is the lowest and min_entries is set to server[i] entries. Otherwise, we compare the number of entires server[i] entries to the local min entries, and if server[i] is lower we set the local min entries equal to server[i]. Finally, we check if the server[i] entries is equal to the local min entries, in which case we compare their success rates as a tiebreaker (`1-(errors/entries)`). If the success rate for server[i] is higher than the local min index success rate, we set the min entries to server[i], otherwise whoever is currently the lowest defaults to staying the lowest.
